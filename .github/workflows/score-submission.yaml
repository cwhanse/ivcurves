name: Score submission

on:
  push:
  workflow_call:
    outputs:
      run-scorer:
        description: 'The pr_config.json says to run the scorer.'
        value: ${{ jobs.read-pr-config.outputs.run-scorer }}
      submission-main:
        description: 'SUBMISSION_MAIN in the pr_config.json.'
        value: ${{ jobs.read-pr-config.outputs.submission-main }}

jobs:
  read-pr-config:
    name: Read pull request config
    runs-on: ubuntu-latest
    outputs:
      contributor: ${{ steps.save-env.outputs.contributor }} # GitHub username of committer or pull request author
      run-scorer: ${{ steps.save-env.outputs.run-scorer }} # Boolean to run the run-scorer job
      requirements-filename: ${{ steps.save-env.outputs.requirements-filename }} # filename of the submission's Pip requirements file (usually requirements.txt)
      requirements-path: ${{ steps.save-env.outputs.requirements-path }} # path to the parent directory that contains requirements-filename
      submission-main-filename: ${{ steps.save-env.outputs.submission-main-filename }} # filename of the file that should be executed by run-scorer
      submission-main-path: ${{ steps.save-env.outputs.submission-main-path }} # path to the parent directory that contains submission-main-filename
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Install Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Save pull request contributor username
        id: save-pr-author
        run: echo '::set-output name=contributor::${{ github.event.pull_request.user.login }}'
      - name: Save GitHub actor username
        id: save-github-actor
        if: ${{ steps.save-pr-author.outputs.contributor == '' }}
        run: echo '::set-output name=contributor::${{ github.actor }}'
      - name: Set the contributor environment variable
        env:
          CONTRIBUTOR: ${{ join(steps.*.outputs.contributor, '') }}
        run: echo 'CONTRIBUTOR=${{ env.CONTRIBUTOR }}' >> ${{ github.env }}
      - name: Set pr_config.json entries as environment variables
        run: python3 .github/workflows/utils/print_json_as_env.py
               submissions/${{ env.CONTRIBUTOR }}/pr_config.json
               --validate-pr-config
               --split-path-variables
             >> ${{ github.env }}
      - name: Save scorer environment to job output
        id: save-env
        run: |
          echo '::set-output name=contributor::${{ env.CONTRIBUTOR }}'
          echo '::set-output name=run-scorer::${{ env.RUN_SCORER }}'
          echo '::set-output name=requirements-filename::${{ env.REQUIREMENTS_FILENAME }}'
          echo '::set-output name=requirements-path::${{ env.REQUIREMENTS_PATH }}'
          echo '::set-output name=submission-main::${{ env.SUBMISSION_MAIN }}'
          echo '::set-output name=submission-main-filename::${{ env.SUBMISSION_MAIN_FILENAME }}'
          echo '::set-output name=submission-main-path::${{ env.SUBMISSION_MAIN_PATH }}'

  run-scorer:
    name: Run scorer
    runs-on: ubuntu-latest
    needs: read-pr-config
    if: ${{ needs.read-pr-config.outputs.run-scorer == 'true' }}
    env:
      CONTRIBUTOR: ${{ needs.read-pr-config.outputs.contributor }}
      REQUIREMENTS_FILENAME: ${{ needs.read-pr-config.outputs.requirements-filename }}
      REQUIREMENTS_PATH: ${{ needs.read-pr-config.outputs.requirements-path }}
      SUBMISSION_MAIN_FILENAME: ${{ needs.read-pr-config.outputs.submission-main-filename }}
      SUBMISSION_MAIN_PATH: ${{ needs.read-pr-config.outputs.submission-main-path }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Install Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install ivcurves requirements
        run: pip3 install -r ivcurves/requirements.txt
      - name: Install submission requirements
        working-directory: submissions/${{ env.CONTRIBUTOR }}/${{ env.REQUIREMENTS_PATH }}
        run: pip3 install -r "${{ env.REQUIREMENTS_FILENAME }}"
      - name: Run submission
        working-directory: submissions/${{ env.CONTRIBUTOR }}/${{ env.SUBMISSION_MAIN_PATH }}
        env:
          SUBMISSION_TIMEOUT: 2 # minutes
        # timeout sends a SIGINT after SUBMISSION_TIMEOUT minutes.
        # if the submission is still running after SIGINT, SIGKILL is sent the next second.
        run: timeout -k 1s ${{ env.SUBMISSION_TIMEOUT }}m python3 "${{ env.SUBMISSION_MAIN_FILENAME }}"
      - name: Run scorer
        working-directory: submissions/${{ env.CONTRIBUTOR }}
        # will fail if no CSV output from the submission,
        # or its CSV files contain very inaccurate results (no intersection between the true and fitted curves can be found)
        run: python3 ../../ivcurves/compare_curves.py "${{ env.SUBMISSION_MAIN_PATH}}" --csv-output-path .
      - name: Validate scores
        run: python3 .github/workflows/utils/record_scores.py
               --overall-scores-path overall_scores.csv
               --no-save-database
      - name: Save scores to artifacts
        uses: actions/upload-artifact@v3
        with:
          name: overall_scores.csv
          path: submissions/${{ env.CONTRIBUTOR }}/overall_scores.csv

